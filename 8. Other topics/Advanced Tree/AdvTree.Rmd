---
title: "Advanced Tree Models -- Bagging, Random Forests, and Boosting"
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
  html_notebook: 
    fig_caption: yes
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this lab, we will cover some state-of-the-art techniques in the framework of tree models.
We use the same datasets as in previous lab, Boston Housing data and Credit Scoring data.
```{r}
# load Boston data
library(MASS)
data(Boston)
index <- sample(nrow(Boston),nrow(Boston)*0.60)
boston.train <- Boston[index,]
boston.test <- Boston[-index,]

# load credit card data
credit.data <- read.csv("data/credit_default.csv", header=T)
# convert categorical variables
credit.data$SEX<- as.factor(credit.data$SEX)
credit.data$EDUCATION<- as.factor(credit.data$EDUCATION)
credit.data$MARRIAGE<- as.factor(credit.data$MARRIAGE)
# random splitting
index <- sample(nrow(credit.data),nrow(credit.data)*0.60)
credit.train = credit.data[index,]
credit.test = credit.data[-index,]
```

# Bagging

*Bagging* stands for Bootstrap and Aggregating. It employs the idea of bootstrap but the purpose is not to study bias and standard errors of estimates. Instead, the goal of Bagging is to improve prediction accuracy. It fits a tree for each bootsrap sample, and then aggregate the predicted values from all these different trees. For more details, you may look at [Wikepedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating), or you can find the original paper [Leo Breiman (1996)](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf).


An available R package, `ipred`, provides functions to perform Bagging. You need to install this package if you didn't do it before.
```{r, warning=FALSE}
library(ipred)
```

## Bagging for regression tree.

Fit tree with bagging on Boston training data, and calculate MSE on testing sample.
```{r}
boston.bag<- bagging(medv~., data = boston.train, nbagg=100)
boston.bag
```

Prediction on testing sample.
```{r}
boston.bag.pred<- predict(boston.bag, newdata = boston.test)
mean((boston.test$medv-boston.bag.pred)^2)
```

Comparing with a single tree.
```{r}
library(rpart)
boston.tree<- rpart(medv~., data = boston.train)
boston.tree.pred<- predict(boston.tree, newdata = boston.test)
mean((boston.test$medv-boston.tree.pred)^2)
```

How many trees are good? 
```{r}
ntree<- c(1, 3, 5, seq(10, 200, 10))
MSE.test<- rep(0, length(ntree))
for(i in 1:length(ntree)){
  boston.bag1<- bagging(medv~., data = boston.train, nbagg=ntree[i])
  boston.bag.pred1<- predict(boston.bag1, newdata = boston.test)
  MSE.test[i]<- mean((boston.test$medv-boston.bag.pred1)^2)
}
plot(ntree, MSE.test, type = 'l', col=2, lwd=2, xaxt="n")
axis(1, at = ntree, las=1)
```

```{r echo=FALSE, eval=FALSE}
ntree<- c(1, 3, 5, seq(10, 200, 10))
MSE.test<- matrix(0, length(ntree), 50)
for(k in 1:50){
  for(i in 1:length(ntree)){
    boston.bag1<- bagging(medv~., data = boston.train, nbagg=ntree[i])
    boston.bag.pred1<- predict(boston.bag1, newdata = boston.test)
    MSE.test[i,k]<- mean((boston.test$medv-boston.bag.pred1)^2)
  }
}
setwd("C:\\Users\\zhuxr\\Dropbox\\Writing Books\\Data-Mining-R\\8. Other topics\\Advanced Tree\\data")
write.csv(MSE.test, file = "Bag_MSE.csv", row.names = FALSE)
MSE.test.ave= apply(MSE.test, 1, mean)
plot(ntree, MSE.test.ave, ylab="MSE.test", type = 'l', col=2, lwd=2)
```

By fitting the Bagging multiple times and predicting the testing sample, we can draw the following boxplot to show the variance of the prediction error at different number of trees.

```{r echo=FALSE}
setwd("data/")
MSE.test= read.csv("Bag_MSE.csv")
ntree<- c(1, 3, 5, seq(10, 200, 10))
boxplot(t(MSE.test), names=ntree, xlab="Number of Tree", ylab="Test MSE")
lines(apply(MSE.test, 1, mean), col="red", lty=2, lwd=2)
```


## Out-of-bag (OOB) prediction

The out-of-bag prediction is similar to LOOCV. We use full sample. In every bootstrap, the unused sample serves as testing sample, and testing error is calculated. In the end, OOB error, root mean squared error by default, is obtained 

```{r}
boston.bag.oob<- bagging(medv~., data = boston.train, coob=T, nbagg=100)
boston.bag.oob
```

## Bagging for classification tree.

To my best knowledge, it seems that `bagging()` won't take an argument for asymmetric loss. Therefore, the classification results might not be appropriate. 

<!-- However, we can still get probabilities. Note that the predicted probabilities from `bagging()` is although the proportion of 1/0 in terminal modes, they are aggregated in the end. Hence, it is unlikely that many observations have the same predicted probability. -->

```{r}
credit.bag<- bagging(as.factor(default.payment.next.month)~., data = credit.train, nbagg=100)
credit.bag.pred<- predict(credit.bag, newdata = credit.train, type="prob")[,2]
credit.bag.pred.test<- predict(credit.bag, newdata = credit.test, type="prob")[,2]
library(ROCR)
pred = prediction(credit.bag.pred.test, credit.test$default.payment.next.month)
perf = performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
unlist(slot(performance(pred, "auc"), "y.values"))

```

The classification results are generated by specifying `type="class"`.

```{r}
credit.bag.pred.test<- predict(credit.bag, newdata = credit.test, type="class")
table(credit.test$default.payment.next.month, credit.bag.pred.test, dnn = c("True", "Pred"))

costMatrix <- matrix(c(0,35,1,0), nrow=2)
credit_asy_bag<- bagging(as.factor(default.payment.next.month)~., 
                         data = credit.train,
                         parms=list(cost=costMatrix), nbagg=100)
credit_asy_bag_pred<- predict(credit_asy_bag, newdata = credit.test)
table(credit.test$default.payment.next.month, credit_asy_bag_pred, dnn = c("True", "Pred"))

```


- Question: What will you see if you calculate the AUC of training set?

```{r , eval=FALSE}
predtrain = prediction(credit.bag.pred, credit.train$default.payment.next.month)
perftrain = performance(predtrain, "tpr", "fpr")
plot(perftrain, colorize=TRUE)
unlist(slot(performance(predtrain, "auc"), "y.values"))

credit.bag.pred.train <- predict(credit.bag, newdata = credit.train, type="class")
table(credit.train$default.payment.next.month, credit.bag.pred.train, dnn = c("True", "Pred"))
```

<!-- ## Something is wrong. This part needs to be investigated -->
<!-- ```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=5} -->
<!-- library(ROCR) -->
<!-- pred <- prediction(credit.bag.pred, credit.train$default.payment.next.month) -->
<!-- perf <- performance(pred, "tpr", "fpr") -->
<!-- plot(perf, colorize=TRUE) -->
<!-- #Get the AUC -->
<!-- unlist(slot(performance(pred, "auc"), "y.values")) -->
<!-- ``` -->



<!-- <!-- Let us choose the optimal cut-off based on our asymmetric loss. --> -->
<!-- ```{r} -->
<!-- costfunc = function(obs, pred.p, pcut){ -->
<!--     weight1 = 5  # define the weight for "true=1 but pred=0" (FN) -->
<!--     weight0 = 1    # define the weight for "true=0 but pred=1" (FP) -->
<!--     c1 = (obs==1)&(pred.p<pcut)    # count for "true=1 but pred=0"   (FN) -->
<!--     c0 = (obs==0)&(pred.p>=pcut)   # count for "true=0 but pred=1"   (FP) -->
<!--     cost = mean(weight1*c1 + weight0*c0)  # misclassification with weight -->
<!--     return(cost) # you have to return to a value when you write R functions -->
<!-- } -->
<!-- p.seq = seq(0.01, 1, 0.01) -->
<!-- cost = rep(0, length(p.seq)) -->
<!-- for(i in 1:length(p.seq)){ -->
<!--     cost[i] = costfunc(obs = credit.train$default.payment.next.month, pred.p = credit.bag.pred, pcut = p.seq[i]) -->
<!-- } -->
<!-- plot(p.seq, cost) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- optimal.pcut = p.seq[which(cost==min(cost))] -->
<!-- credit.bag.class<- (credit.bag.pred.test>optimal.pcut)*1 -->
<!-- table(credit.test$default.payment.next.month, credit.bag.class, dnn = c("True", "Pred")) -->
<!-- ``` -->

Comparing with a single tree. For fair comparison, we assume the loss is symmetric.

```{r}
credit.rpart <- rpart(formula = default.payment.next.month ~ ., data = credit.train, method = "class")
credit.test.pred.tree1 <- predict(credit.rpart, credit.test, type="class")
table(credit.test$default.payment.next.month, credit.test.pred.tree1, dnn=c("Truth","Predicted"))
credit.test.pred.tree1 <- predict(credit.rpart, credit.test, type="prob")

pred = prediction(credit.test.pred.tree1[,2], credit.test$default.payment.next.month)
perf = performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
unlist(slot(performance(pred, "auc"), "y.values"))
```


# Random Forests

Random forest is an extension of Bagging, but it makes significant improvement in terms of prediction. The idea of random forests is to randomly select $m$ out of $p$ predictors as candidate variables for each split in each tree. Commonly, $m=\sqrt{p}$. The reason of doing this is that it can *decorrelates* the trees such that it reduces variance when we aggregate the trees. You may refer [Wikipedia](https://en.wikipedia.org/wiki/Random_forest) and the [tutorial](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) on the author's website.

## Random Forest for Regression

We start with Boston Housing data.

```{r warning=FALSE, message=FALSE}
library(randomForest)
boston.rf<- randomForest(medv~., data = boston.train, importance=TRUE)
boston.rf
costMatrix <- matrix(c(0,10,1,0), nrow=2)
mod_rf <- randomForest(medv~., data=boston.train, 
                       importance=TRUE, ntree=500, 
                       parms = list(loss=costMatrix))

```

By default, $m=p/3$ for regression tree, and $m=\sqrt{p}$ for classification problem. You can change it by specifying `mtry=`. You can also specify number of trees by `ntree=`. The default is 500.
The argument `importance=TRUE` allows us to see the variable imporatance.

```{r}
boston.rf$importance
mod_rf$importance
```

The MSR is MSE of *out-of-bag* prediction (recall the OOB in bagging).  The fitted randomForest actually saves all OOB errors for each `ntree` value from 1 to 500. We can make a plot to see how the OOB error changes with different `ntree`. 

```{r}
plot(boston.rf$mse, type='l', col=2, lwd=2, xlab = "ntree", ylab = "OOB Error")
plot(mod_rf$mse, type='l', col=2, lwd=2, xlab = "ntree", ylab = "OOB Error")
```

Prediction on the testing sample.
```{r}
boston.rf.pred<- predict(boston.rf, boston.test)
mean((boston.test$medv-boston.rf.pred)^2)
```


As we mentioned before, the number of candidate predictors in each split is $m\approx \sqrt{p}=\sqrt{13}\approx 4$. We can also specify $m$ with argument `mtry`. Now let's see how the OOB error and testing error changes with `mtry`.

```{r}
oob.err<- rep(0, 13)
test.err<- rep(0, 13)
for(i in 1:13){
  fit<- randomForest(medv~., data = boston.train, mtry=i)
  oob.err[i]<- fit$mse[500]
  test.err[i]<- mean((boston.test$medv-predict(fit, boston.test))^2)
  cat(i, " ")
}
matplot(cbind(test.err, oob.err), pch=15, col = c("red", "blue"), type = "b", ylab = "MSE", xlab = "mtry")
legend("topright", legend = c("test Error", "OOB Error"), pch = 15, col = c("red", "blue"))
```

#### Exercise: Create a plot displaying the test error across `ntree=`1, ..., 500, and `mtry=` 1, ..., 13. (You can draw 13 lines in different color representing each $m$).

## Random Forest for Classification

```{r}
credit.rf <- randomForest(as.factor(default.payment.next.month)~., data = credit.train)
credit.rf
costMatrix <- matrix(c(0,100,1,0), nrow=2)
credit_asy_rf <- randomForest(as.factor(default.payment.next.month)~.,
                              data=credit.train, 
                              importance=TRUE, ntree=500, 
                              parms = list(loss=costMatrix))
credit_asy_rf
```

We can again easily plot the error rate vs. ntree. However, as far as I know, `randomForest` does not support asymmetric loss either. So it always uses the overall misclassification rate as the error.
```{r}
plot(credit.rf, lwd=rep(2, 3))
legend("right", legend = c("OOB Error", "FPR", "FNR"), lwd=rep(2, 3), lty = c(1,2,3), col = c("black", "red", "green"))
```

As we can see, the FNR is very high, just as the confusion matrix. For prediction purpose, nevertheless, we can use `type="prob"` to get predicted probability and then find optimal cut-off. Let's do in-sample prediction and find optimal cut-off.
```{r}
credit.rf.pred<- predict(credit.rf, type = "prob")[,2]
costfunc = function(obs, pred.p, pcut){
    weight1 = 5   # define the weight for "true=1 but pred=0" (FN)
    weight0 = 1    # define the weight for "true=0 but pred=1" (FP)
    c1 = (obs==1)&(pred.p<pcut)    # count for "true=1 but pred=0"   (FN)
    c0 = (obs==0)&(pred.p>=pcut)   # count for "true=0 but pred=1"   (FP)
    cost = mean(weight1*c1 + weight0*c0)  # misclassification with weight
    return(cost) # you have to return to a value when you write R functions
} 
p.seq = seq(0.01, 0.5, 0.01)
cost = rep(0, length(p.seq))  
for(i in 1:length(p.seq)){ 
    cost[i] = costfunc(obs = credit.train$default.payment.next.month, pred.p = credit.rf.pred, pcut = p.seq[i])  
}
plot(p.seq, cost)
```

ROC curve and AUC is can be obtained based on the probability prediction. (Not sure why Bagging does not work.)
```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
library(ROCR)
pred <- prediction(credit.rf.pred, credit.train$default.payment.next.month)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
```

Create the confusion matrix based on the optimal cutoff probability.
```{r}
## out-of-sample
optimal.pcut= p.seq[which(cost==min(cost))]
credit.rf.pred.test<- predict(credit.rf, newdata=credit.test, type = "prob")[,2]
credit.rf.class.test<- (credit.rf.pred.test>optimal.pcut)*1
table(credit.test$default.payment.next.month, credit.rf.class.test, dnn = c("True", "Pred"))
```


# Boosting

Boosting builds a number of small trees, and each time, the response is the residual from last tree. It is a sequential procedure. We use `gbm` package to build boosted trees.

## Boosting for regression trees

```{r warning=FALSE, message=FALSE}
library(gbm)
?gbm
boston.boost<- gbm(medv~., data = boston.train, distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 8)
summary(boston.boost)
```

Note that we need to specify `distribution = "gaussian"` if we are working on regression tree. The default is *Bernoulli* distribution for binary classification problem. `n.trees` is the number of small trees we fit. We need to choose this parameter carefully because it may results in overfitting if the number is too large. `shrinkage` is another tuning parameter that controls how much contribution each tree makes. `interaction.depth` is how many splits of each tree we want. All those tuning parameters can be chosen from cross-validation. The idea is that we don't want overfitting.

The fitted boosted tree also gives the relation between response and each predictor.
```{r}
par(mfrow=c(1,2))
plot(boston.boost, i="lstat")
plot(boston.boost, i="rm")
```

Prediction on testing sample.
```{r}
boston.boost.pred.test<- predict(boston.boost, boston.test, n.trees = 10000)
mean((boston.test$medv-boston.boost.pred.test)^2)
```

We can investigate how the testing error changes with different number of trees.
```{r}
ntree<- seq(100, 10000, 100)
predmat<- predict(boston.boost, newdata = boston.test, n.trees = ntree)
err<- apply((predmat-boston.test$medv)^2, 2, mean)
plot(ntree, err, type = 'l', col=2, lwd=2, xlab = "n.trees", ylab = "Test MSE")
abline(h=min(test.err), lty=2)
```

The horizontal line is the best prediction error from random forests we obtained earlier.

## Boosting for classification trees

```{r, eval=FALSE}
library(adabag)
credit.train$default.payment.next.month= as.factor(credit.train$default.payment.next.month)
credit.boost= boosting(default.payment.next.month~., data = credit.train, boos = T)
save(credit.boost, file = "credit.boost.Rdata")

# Training AUC
pred.credit.boost= predict(credit.boost, newdata = credit.train)
pred <- prediction(pred.credit.boost$prob[,2], credit.train$default.payment.next.month)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))

pred.credit.boost= predict(credit.boost, newdata = credit.test)
# Testing AUC
pred <- prediction(pred.credit.boost$prob[,2], credit.test$default.payment.next.month)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))

```
