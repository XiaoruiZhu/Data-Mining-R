---
title: "Regression Trees"
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
---

In this lab we will go through the model building, validation, and interpretation of tree models. The focus will be on *rpart* package.

# CART: classification and regression trees

CART stands for classification and regression trees:

* Regression trees: response variable Y is numerical
* Classification trees: response variable Y is categorical

For the regression trees example, we will use the Boston Housing data. Recall the response variable is the housing price. For the classification trees example, we will use the credit scoring data. The response variable is whether the loan went to default.

Note that unlkie logistic regreesion, the response variable does not have to be binary in case of classification trees. We can use classification trees on classification problems with more than 2 outcomes.

[go to top](#header)

## Regression Trees (Boston Housing Data)

Let us load the data sets. Random sampled training and test datasets will lead to different results,

```{r}
library(MASS) #this data is in MASS package
boston.data <- data(Boston)
sample_index <- sample(nrow(Boston),nrow(Boston)*0.90)
boston.train <- Boston[sample_index,]
boston.test <- Boston[-sample_index,]
```

We will use the 'rpart' library for model building and 'rpart.plot' for plotting.
```{r, eval=FALSE}
install.packages('rpart')
install.packages('rpart.plot') 
```

```{r,warning=FALSE}
library(rpart)
library(rpart.plot)
```

The simple form of the *rpart* function is similar to *lm* and *glm*. It takes a formula argument in which you specify the response and predictor variables, and a data argument in which you specify the data frame.

```{r}
boston.rpart <- rpart(formula = medv ~ ., data = boston.train)
```

### Printing and ploting the tree
```{r, message=FALSE,echo=FALSE}
options(digits=7)
```

```{r}
boston.rpart
prp(boston.rpart,digits = 4, extra = 1)
```

Make sure you know how to interpret this tree model!

**Exercise:** What is the predicted median housing price (in thousand) given following information:
```{r, echo=FALSE}
library(knitr)
kable(boston.test[10,], digits=2, row.names = F)
```

### Prediction using regression trees

The in-sample and out-of-sample prediction for regression trees is also similar to *lm* and *glm* models.

- In-sample prediction
```{r}
boston.train.pred.tree = predict(boston.rpart)
```

- Out-of-sample prediction
```{r}
boston.test.pred.tree = predict(boston.rpart,boston.test)
```

We often denote MSE as training error, and MSPE as testing error when sample size is large. 
**Exercise:** Calculate the mean squared error (MSE) for this tree model
```{r, eval=FALSE}
MSE.tree<- 
MSPE.tree <- 
```

<!-- The mean squred error loss for this tree model is -->
<!-- ```{r} -->
<!-- MSE.tree <- mean((boston.train.pred.tree - boston.train$medv)^2) -->
<!-- MSPE.tree <- mean((boston.test.pred.tree - boston.test$medv)^2) -->
<!-- ``` -->

We can compare this model's out-of-sample performance with the linear regression model with all variables in it.

```{r}
boston.reg = lm(medv~., data = boston.train)
boston.test.pred.reg = predict(boston.reg, boston.test)
mean((boston.test.pred.reg - boston.test$medv)^2)
```

### Comparing the performance of regression trees with linear regression model in terms of prediction error (Exercise)

Calculate the mean squared error (MSE) and mean squared prediction error (MSPE) for linear regression model using all variables. Then compare the results. What is your conclusion? Further, try to compare the regression trees with the best linear regression model using some variable selection procedures.

```{r, eval=FALSE}
boston.lm<- 
boston.train.pred.lm<- 
boston.test.pred.lm<- 
MSE.lm<- 
MSPE.lm<-
```


## Pruning

In rpart(), the cp(complexity parameter) argument is one of the parameters that are used to control the compexity of the tree. The help document for rpart tells you "Any split that does not decrease the overall lack of fit by a factor of cp is not attempted". For a regression tree, the overall R-square must increase by cp at each step. Basically, the smaller the cp value, the larger (complex) tree rpart will attempt to fit.  The default value for cp is 0.01.

What happens when you have a large tree? The following tree has 27 splits.  

```{r}
boston.largetree <- rpart(formula = medv ~ ., data = boston.train, cp = 0.001)
```

Try plot it yourself to see its structure.
```{r, eval=FALSE}
prp(boston.largetree)
```

The plotcp() function gives the relationship between 10-fold cross-validation error in the training set and size of tree.
```{r}
plotcp(boston.largetree)
```

You can observe from the above graph that the cross-validation error (x-val) does not always go down when the tree becomes more complex. The analogy is when you add more variables in a regression model, its ability to predict future observations not necessarily increases. A good choice of cp for pruning is often the leftmost value for which the mean lies below the horizontal line. In the Boston housing example, you may conclude that having a tree mode with more than 10 splits is not helptul.

To look at the error vs size of tree more carefully, you can look at the following table:
```{r}
printcp(boston.largetree)
```

Root node error is the error when you do not do anything too smart in prediction, in regression case, it is the mean squared error(MSE) if you use the average of medv as the prediction. Note it is the same as
```{r}
sum((boston.train$medv - mean(boston.train$medv))^2)/nrow(boston.train)
```
The first 2 columns CP and nsplit tells you how large the tree is. rel.error $\times$ root node error gives you the in sample error. For example, The last row `(rel error)*(root node error)= 0.13085*87.133 = 11.40135`, which is the same as the in-sample MSE if you calculate using predict:

```{r}
mean((predict(boston.largetree) - boston.train$medv)^2)
```
xerror gives you the cross-validation (default is 10-fold) error. You can see that the rel error (in-sample error) is always decreasing as model is more complex, while the cross-validation error (measure of performance on future observations) is not. That is why we **prune** the tree to avoid overfitting the training data.

The way rpart() does it is that it uses some default control parameters to avoid fitting a large tree. The main reason for this approach is to save computation time. For example by default rpart set a cp = 0.1 and the minimum number of observations that must exist in a node to be 20. Use ?rpart.control to view these parameters. Sometimes we wish to change these paramters to see how more complex trees will perform, as we did above. If we have a larger than necessary tree, we can use prune() function and specify a new cp:
```{r}
prune(boston.largetree, cp = 0.008)
```

**Exercise:** Prune a classification tree. Start with "cp=0.001", and find a reasonable cp value, then obtain the pruned tree.

Some software/packages can automatically prune the tree.

[go to top](#header)

