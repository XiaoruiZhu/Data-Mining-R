---
title: "Neural Networks Models"
header-includes:
   - \usepackage{float}
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
    code_folding: show
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```
```{r library, eval=TRUE, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(xtable)
library(glmnet)
```

# Objective

This tutorial helps you to review various supervised learning techniques, introduce GAM, Neural Networks models, etc.

# Neural Networks Models
Neural Networks method (in-sample and out-of-sample performance measure) is illustrated here. The package [**nnet**](http://cran.r-project.org/web/packages/nnet/nnet.pdf) is used for this purpose.

## Regression

__Note__: 

- For regression problems add lineout = TRUE when training model. In addition, the response needs to be standardized to $[0, 1]$ interval. It's important normalize the response. If not, most of the times the algorithm will not converge. I chose to use the min-max method and scale the data in the interval [0,1]. 

```{r, message=FALSE}
library(MASS)
maxs <- apply(Boston, 2, max) 
mins <- apply(Boston, 2, min)

scaled <- as.data.frame(scale(Boston, center = mins, scale = maxs - mins))
index <- sample(1:nrow(Boston),round(0.9*nrow(Boston)))

train_Boston <- scaled[index,]
test_Boston <- scaled[-index,]

library(neuralnet)
```

```{r, fig.width=7}
n <- names(train_Boston)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train_Boston, hidden=c(5,3), linear.output=T)
plot(nn)
```

```{r}
pr_nn <- compute(nn, test_Boston[,1:13])

pr_nn_org <- pr_nn$net.result*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
test_r <- (test_Boston$medv)*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
# MSE of testing set
MSE_nn <- sum((test_r - pr_nn_org)^2)/nrow(test_Boston)
MSE_nn
```

## Classification

__Note__: 

- For classification problems with nnet you need to code the response to _factor_ first. In addition you want to add type = "class" for _predict()_  function. 

### Credit Score Data

```{r}
# credit_data <- read.csv("data/credit0.csv", header=T)
credit_data <- read.csv(file = "https://xiaoruizhu.github.io/Data-Mining-R/lecture/data/credit0.csv", header=T)
```

We remove X9 and id from the data since we will not be using them for prediction.
```{r}
credit_data$X9 <- NULL
credit_data$id <- NULL
credit_data$Y <- as.factor(credit_data$Y)
```

Now split the data 90/10 as training/testing datasets:
```{r}
id_train <- sample(nrow(credit_data),nrow(credit_data)*0.90)
credit_train <- credit_data[id_train,]
credit_test <- credit_data[-id_train,]
```

The training dataset has 61 variables, 4500 obs. 

You are already familiar with the credit scoring set. Let's define a cost function for benchmarking testing set performance. Note this is slightly different from the one we used for searching for optimal cut-off probability in logistic regression. Here the 2nd argument is the predict class instead of the predict probability (since many methods are not based on predict probability).

```{r}
creditcost <- function(observed, predicted){
  weight1 <- 10
  weight0 <- 1
  c1 <- (observed==1)&(predicted == 0) #logical vector - true if actual 1 but predict 0
  c0 <- (observed==0)&(predicted == 1) #logical vector - true if actual 0 but predict 1
  return(mean(weight1*c1+weight0*c0))
}
```

```{r, message=FALSE}
library(nnet)
```

```{r}
credit_nnet <- nnet(Y~., data=credit_train, size=5, maxit=500)
```

```{r}
prob_nnet <- predict(credit_nnet, credit_test)
pred_nnet <- as.numeric(prob_nnet > 0.1)
table(credit_test$Y,pred_nnet, dnn=c("Observed","Predicted"))
cat("Misclassification rate =", mean(ifelse(credit_test$Y != pred_nnet, 1, 0)))
cat("Asymmetric cost (with ratio 10:1) =", creditcost(credit_test$Y, pred_nnet))
```

[go to top](#header)
