---
title: "Moving Beyond Linearity"
header-includes:
   - \usepackage{float}
output: 
  html_document: 
    keep_md: yes
    number_sections: yes
    toc: yes
  latex_notebook: default
  pdf_document: 
    number_sections: yes
editor_options: 
  chunk_output_type: console
---


```r
library(MASS)
data('mcycle')
str(mcycle)
summary(mcycle)
```


```r
# Rename the variables for ease of usage
Y <- mcycle$accel
X <- mcycle$times

#Scatterplot
plot(Y~X, xlab="time",ylab="Acceleration", main="Scatterplot of Acceleration against Time")
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-2-1.png)<!-- -->


# Simple Linear Regression


```r
lm_mod <- lm(Y~X, data= mcycle)
summary(lm_mod)
```


**Fitted Regression Line**


```r
plot(X, Y, xlab="Times", ylab="Acceleration", main="Simple Linear Regression Line")
abline(lm_mod, col="blue", lwd = 1)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-4-1.png)<!-- -->


# Polynomial Regression

The coefficients can be easily estimated using least squares linear regression because this is just a standard linear model with predictors x, x^2, x^3, ...,x^d. 

## 1.1 Quadratic


```r
quad_mod <- lm(Y~X+I(X^2), data=mcycle) 
summary(quad_mod)
```

```
## 
## Call:
## lm(formula = Y ~ X + I(X^2), data = mcycle)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -96.527 -30.817   9.589  29.210 104.728 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)   
## (Intercept) -15.22700   15.49471  -0.983  0.32757   
## X            -2.30749    1.20439  -1.916  0.05757 . 
## I(X^2)        0.05935    0.02038   2.912  0.00422 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 45.06 on 130 degrees of freedom
## Multiple R-squared:  0.1437,	Adjusted R-squared:  0.1306 
## F-statistic: 10.91 on 2 and 130 DF,  p-value: 4.167e-05
```



```r
plot(X ,Y ,xlab="Times", main = "Quadratic",ylab="Acceleration",cex=.5)
lines(X,quad_mod$fitted.values, col="blue", lwd = 1)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-6-1.png)<!-- -->

**Is this model superior to the simple linear regression model?** 


```r
anova(lm_mod,quad_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: Y ~ X
## Model 2: Y ~ X + I(X^2)
##   Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
## 1    131 281144                                
## 2    130 263923  1     17221 8.4823 0.004222 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```



## 1.2 Fifth-degree Polynomial


```r
poly_mod <- lm(Y~poly(X,5,raw=T),data=mcycle) 
summary(poly_mod)
```

```
## 
## Call:
## lm(formula = Y ~ poly(X, 5, raw = T), data = mcycle)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -77.271 -21.285   0.975  25.386  82.371 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)          -1.059e+02  3.499e+01  -3.026    0.003 ** 
## poly(X, 5, raw = T)1  4.978e+01  1.036e+01   4.804 4.31e-06 ***
## poly(X, 5, raw = T)2 -6.359e+00  1.015e+00  -6.266 5.30e-09 ***
## poly(X, 5, raw = T)3  2.969e-01  4.253e-02   6.982 1.45e-10 ***
## poly(X, 5, raw = T)4 -5.742e-03  7.932e-04  -7.239 3.83e-11 ***
## poly(X, 5, raw = T)5  3.919e-05  5.406e-06   7.250 3.60e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 33.9 on 127 degrees of freedom
## Multiple R-squared:  0.5264,	Adjusted R-squared:  0.5078 
## F-statistic: 28.24 on 5 and 127 DF,  p-value: < 2.2e-16
```


You can also assess the model performance.


```r
#poly_mod_summary <- summary(poly_mod)
#(poly_mod_summary$sigma)^2 
#poly_mod_summary$r.squared
#poly_mod_summary$adj.r.squared
#AIC(poly_mod)
#BIC(poly_mod)
```



```r
plot(X ,Y ,xlab="Times", main = "Fifth-degree polynomial",ylab="Acceleration",cex=.5)
lines(X,poly_mod$fitted.values, col="blue", lwd = 1)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-10-1.png)<!-- -->

[go to top](#header)

# Splines

## 2.1 Regression Splines

In order to fit regression splines in R, we use the bs() function from the splines library. By default, "cubic splines" are produced. That is cubic polynomial with no interior knots


```r
library (splines)
reg_sp <- lm(Y~bs(X),data=mcycle)
summary(reg_sp)
```

```
## 
## Call:
## lm(formula = Y ~ bs(X), data = mcycle)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -83.083 -28.772   2.935  31.004  94.108 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)    41.26      15.42   2.675 0.008433 ** 
## bs(X)1       -258.14      41.07  -6.285 4.66e-09 ***
## bs(X)2        110.67      28.33   3.906 0.000151 ***
## bs(X)3        -72.91      27.99  -2.605 0.010274 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 40 on 129 degrees of freedom
## Multiple R-squared:  0.3303,	Adjusted R-squared:  0.3147 
## F-statistic: 21.21 on 3 and 129 DF,  p-value: 3.134e-11
```



```r
plot(X ,Y ,xlab="Times", main = "Regression Spline",ylab="Acceleration",cex=.5)
lines(X,reg_sp$fitted.values, col="blue", lwd = 1)

conf_interval <- predict(reg_sp, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-12-1.png)<!-- -->


You can also specify the knot locations.


```r
#fit_sp=lm(Y~bs(X,knots=c(15.6,23.4,34.8)),data=mcycle) 
#summary(fit_sp)
#AIC(fit_sp)
```

You can also specify the degree of freedom.


```r
reg_sp2=lm(Y~bs(X,df=10),data=mcycle) 

plot(X ,Y ,xlab="Times", main = "Regression Spline with df=10",ylab="Acceleration",cex=.5)
lines(X,reg_sp2$fitted.values, col="blue", lwd = 1)

conf_interval <- predict(reg_sp2, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-14-1.png)<!-- -->

## 2.2 Natural Cubic Splines

Here the degree of freedom is pre-specified and different numbers are used to see the the best curve that fits the data.


```r
#First: Natural Spline- pre-specified degree of freedom=4
fit2=lm(Y~ns(X,df=4),data=mcycle) 
plot(X ,Y,main= "Natural Cubic Spline with df=4", xlab="Times", ylab="Acceleration") 
lines(X, fit2$fitted.values)

conf_interval <- predict(fit2, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-15-1.png)<!-- -->



```r
fit2c=lm(Y~ns(X,df=10),data=mcycle) 

plot(X ,Y , main= "Natural Cubic Spline with df=10", xlab="Times", ylab="Acceleration") 
lines(X, fit2c$fitted.values)

conf_interval <- predict(fit2c, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-16-1.png)<!-- -->


```r
fit2d=lm(Y~ns(X,df=20),data=mcycle) 

plot(X ,Y, main= "Natural Cubic Spline with df=20", xlab="Times", ylab="Acceleration") 
lines(X, fit2d$fitted.values)

conf_interval <- predict(fit2d, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-17-1.png)<!-- -->


## 2.3 Smoothing/Penalized Spline

We now use penalized splines where a penalty/smoothing parameter can help control the smoothness while many knots can be used and knot location does not need to be carefully selected. The s() function is part of the gam function from the mgcv package.

[go to top](#header)

# Generalized Additive Model
## 3.1 GAM using the "Motorcycle" dataset


```r
library(mgcv)
s_gam <- gam(Y ~ s(X),data=mcycle)
summary(s_gam)
```

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## Y ~ s(X)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -25.546      1.951   -13.1   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##        edf Ref.df     F p-value    
## s(X) 8.693  8.972 53.52  <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.783   Deviance explained = 79.8%
## GCV = 545.78  Scale est. = 506       n = 133
```

```r
#plot the model
plot(s_gam, residuals = TRUE, pch = 1)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-18-1.png)<!-- -->

## 3.2 GAM on "Wage" dataset

This dataset contains Wage and other data for a group of 3,000 male workers in the Mid-Atlantic region.
First we conduct EDA and fit a multiple linear regression of wage on age, year, and education.


```r
require(ISLR)
attach(Wage)
# ?Wage
dim(Wage)
```

```
## [1] 3000   11
```

```r
s_Wage <- Wage[c("age","year","education","wage")]
str(s_Wage)
```

```
## 'data.frame':	3000 obs. of  4 variables:
##  $ age      : int  18 24 45 43 50 54 44 30 41 52 ...
##  $ year     : int  2006 2004 2003 2003 2005 2008 2009 2008 2006 2004 ...
##  $ education: Factor w/ 5 levels "1. < HS Grad",..: 1 4 3 4 2 4 3 3 3 2 ...
##  $ wage     : num  75 70.5 131 154.7 75 ...
```

```r
summary(s_Wage)
```

```
##       age             year                   education        wage       
##  Min.   :18.00   Min.   :2003   1. < HS Grad      :268   Min.   : 20.09  
##  1st Qu.:33.75   1st Qu.:2004   2. HS Grad        :971   1st Qu.: 85.38  
##  Median :42.00   Median :2006   3. Some College   :650   Median :104.92  
##  Mean   :42.41   Mean   :2006   4. College Grad   :685   Mean   :111.70  
##  3rd Qu.:51.00   3rd Qu.:2008   5. Advanced Degree:426   3rd Qu.:128.68  
##  Max.   :80.00   Max.   :2009                            Max.   :318.34
```



```r
library(ggplot2)
ggplot(data = Wage, mapping = aes(x = age, y = wage)) + 
    geom_point()
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-20-1.png)<!-- -->

```r
ggplot(data = Wage, mapping = aes(x = year, y = wage)) + 
    geom_point()
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-20-2.png)<!-- -->

```r
#categorical variable
ggplot(Wage, aes(x = education, y = wage)) +
    geom_boxplot()
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-20-3.png)<!-- -->


```r
#multiple linear regression model
mul_model <- lm(wage~age+year+education, data=Wage)
summary(mul_model)
```


We then fit a GAM where 'age' and 'year' enter the model nonlinearly, whereas variable 'education' enters linearly.


```r
gam_mod <- gam(wage~s(age, k=6)+s(year, k=6)+education ,data = Wage)
summary(gam_mod)
```

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## wage ~ s(age, k = 6) + s(year, k = 6) + education
## 
## Parametric coefficients:
##                             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)                   85.456      2.153  39.692  < 2e-16 ***
## education2. HS Grad           10.978      2.428   4.521  6.4e-06 ***
## education3. Some College      23.530      2.558   9.197  < 2e-16 ***
## education4. College Grad      38.159      2.543  15.005  < 2e-16 ***
## education5. Advanced Degree   62.559      2.760  22.668  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F  p-value    
## s(age)  4.435  4.840 46.69  < 2e-16 ***
## s(year) 1.101  1.195 11.11 0.000379 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =   0.29   Deviance explained = 29.2%
## GCV = 1240.7  Scale est. = 1236.3    n = 3000
```

```r
plot(gam_mod, se=TRUE ,col="blue")
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-22-1.png)<!-- -->![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-22-2.png)<!-- -->


**Basis dimension choice for smooths**

*gam.check* runs a simple simulation based check on the basis dimensions, which can help to flag up terms for which k is too low. Grossly too small k will also be visible from partial residuals available with plot.gam.


```r
gam.check(gam_mod)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-23-1.png)<!-- -->

```
## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 7 iterations.
## The RMS GCV score gradient at convergence was 0.002902862 .
## The Hessian was positive definite.
## Model rank =  15 / 15 
## 
## Basis dimension (k) checking results. Low p-value (k-index<1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##           k'  edf k-index p-value
## s(age)  5.00 4.44    0.98    0.14
## s(year) 5.00 1.10    1.02    0.75
```

## 3.3 GAM on Boston Housing dataset


```r
library(MASS)
set.seed(1234)
sample_index <- sample(nrow(Boston),nrow(Boston)*0.70)
Boston_train <- Boston[sample_index,]
Boston_test <- Boston[-sample_index,]
str(Boston_train)
```


**Model and plots**



```r
#create gam model

Boston.gam <- gam(medv ~ s(crim)+s(zn)+s(indus)+chas+s(nox)
                 +s(rm)+s(age)+s(dis)+rad+s(tax)+s(ptratio)
                 +s(black)+s(lstat),data=Boston_train)

summary(Boston.gam)
```

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## medv ~ s(crim) + s(zn) + s(indus) + chas + s(nox) + s(rm) + s(age) + 
##     s(dis) + rad + s(tax) + s(ptratio) + s(black) + s(lstat)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  18.9672     1.4638  12.958   <2e-16 ***
## chas          1.0098     0.7104   1.421   0.1562    
## rad           0.3454     0.1505   2.296   0.0224 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##              edf Ref.df      F  p-value    
## s(crim)    4.543  5.488  7.436 6.38e-07 ***
## s(zn)      1.000  1.000  0.973 0.324644    
## s(indus)   6.701  7.629  4.124 0.000171 ***
## s(nox)     8.946  8.995 17.931  < 2e-16 ***
## s(rm)      7.792  8.605 19.944  < 2e-16 ***
## s(age)     2.415  3.058  1.302 0.282947    
## s(dis)     8.742  8.975  8.895 4.36e-12 ***
## s(tax)     6.122  7.145  7.091 7.01e-08 ***
## s(ptratio) 1.000  1.000 21.257 5.90e-06 ***
## s(black)   1.000  1.000  0.002 0.963662    
## s(lstat)   6.362  7.484 18.551  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.893   Deviance explained =   91%
## GCV = 10.713  Scale est. = 8.9693    n = 354
```

```r
plot(Boston.gam, pages=1)
```

![](6.A_BeyondLinearity_files/figure-html/unnamed-chunk-25-1.png)<!-- -->


**Model AIC/BIC and mean residual deviance**


```r
AIC(Boston.gam)
BIC(Boston.gam)
Boston.gam$deviance
```


**In-sample fit performance**


```r
#in-sample mse using df 
Boston.gam.mse.train <- Boston.gam$dev/Boston.gam$df.residual 
#Average Sum of Squared Error
Boston.gam.mse.train <- Boston.gam$dev/nrow(Boston_train) 

#using the predict() function
pi <- predict(Boston.gam,Boston_train)
mean((pi - Boston_train$medv)^2)
```

```
## [1] 7.509325
```

**out of sample performance**

```r
pi.out <- predict(Boston.gam,Boston_test)
mean((pi.out - Boston_test$medv)^2)
```

```
## [1] 14.04571
```

[go to top](#header)
