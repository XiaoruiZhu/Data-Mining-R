---
title: "Moving Beyond Linearity"
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
    code_folding: show
    df_print: paged
  html_notebook: 
    fig_caption: yes
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: console
---


```{r echo=TRUE, message = FALSE, results='hide'}
library(MASS)
data('mcycle')
str(mcycle)
summary(mcycle)
```

```{r echo=TRUE, message = FALSE}
# Rename the variables for ease of usage
Y <- mcycle$accel
X <- mcycle$times

#Scatterplot
plot(Y~X, xlab="time",ylab="Acceleration", main="Scatterplot of Acceleration against Time")
```


# Simple Linear Regression

```{r echo=TRUE, message = FALSE, results='hide'}
lm_mod <- lm(Y~X, data= mcycle)
summary(lm_mod)
```


**Fitted Regression Line**

```{r echo=TRUE, message = FALSE}
plot(X, Y, xlab="Times", ylab="Acceleration", main="Simple Linear Regression Line")
abline(lm_mod, col="blue", lwd = 1)
```


# Polynomial Regression

The coefficients can be easily estimated using least squares linear regression because this is just a standard linear model with predictors x, x^2, x^3, ...,x^d. 

## 1.1 Quadratic

```{r echo=TRUE, message = FALSE}
quad_mod <- lm(Y~X+I(X^2), data=mcycle) 
summary(quad_mod)
```


```{r echo=TRUE, message = FALSE}
plot(X ,Y ,xlab="Times", main = "Quadratic",ylab="Acceleration",cex=.5)
lines(X,quad_mod$fitted.values, col="blue", lwd = 1)
```

**Is this model superior to the simple linear regression model?** 

```{r echo=TRUE, message = FALSE}
anova(lm_mod,quad_mod)
```



## 1.2 Fifth-degree Polynomial

```{r echo=TRUE, message = FALSE}
poly_mod <- lm(Y~poly(X,5,raw=T),data=mcycle) 
summary(poly_mod)
```


You can also assess the model performance.

```{r echo=TRUE, message = FALSE}
#poly_mod_summary <- summary(poly_mod)
#(poly_mod_summary$sigma)^2 
#poly_mod_summary$r.squared
#poly_mod_summary$adj.r.squared
#AIC(poly_mod)
#BIC(poly_mod)
```


```{r echo=TRUE, message = FALSE}
plot(X ,Y ,xlab="Times", main = "Fifth-degree polynomial",ylab="Acceleration",cex=.5)
lines(X,poly_mod$fitted.values, col="blue", lwd = 1)
```

[go to top](#header)

# Splines

## 2.1 Regression Splines

In order to fit regression splines in R, we use the bs() function from the splines library. By default, "cubic splines" are produced. That is cubic polynomial with no interior knots

```{r echo=TRUE, message = FALSE, warning= FALSE}
library (splines)
reg_sp <- lm(Y~bs(X),data=mcycle)
summary(reg_sp)
```


```{r echo=TRUE, message = FALSE, warning= FALSE}
plot(X ,Y ,xlab="Times", main = "Regression Spline",ylab="Acceleration",cex=.5)
lines(X,reg_sp$fitted.values, col="blue", lwd = 1)

conf_interval <- predict(reg_sp, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```


You can also specify the knot locations.

```{r echo=TRUE, message = FALSE, warning= FALSE}
#fit_sp=lm(Y~bs(X,knots=c(15.6,23.4,34.8)),data=mcycle) 
#summary(fit_sp)
#AIC(fit_sp)
```

You can also specify the degree of freedom.

```{r echo=TRUE, message = FALSE, warning= FALSE}
reg_sp2=lm(Y~bs(X,df=10),data=mcycle) 

plot(X ,Y ,xlab="Times", main = "Regression Spline with df=10",ylab="Acceleration",cex=.5)
lines(X,reg_sp2$fitted.values, col="blue", lwd = 1)

conf_interval <- predict(reg_sp2, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

## 2.2 Natural Cubic Splines

Here the degree of freedom is pre-specified and different numbers are used to see the the best curve that fits the data.

```{r echo=TRUE, message = FALSE, warning=FALSE}
#First: Natural Spline- pre-specified degree of freedom=4
fit2=lm(Y~ns(X,df=4),data=mcycle) 
plot(X ,Y,main= "Natural Cubic Spline with df=4", xlab="Times", ylab="Acceleration") 
lines(X, fit2$fitted.values)

conf_interval <- predict(fit2, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```


```{r echo=TRUE, message = FALSE, warning=FALSE}
fit2c=lm(Y~ns(X,df=10),data=mcycle) 

plot(X ,Y , main= "Natural Cubic Spline with df=10", xlab="Times", ylab="Acceleration") 
lines(X, fit2c$fitted.values)

conf_interval <- predict(fit2c, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```

```{r echo=TRUE, message = FALSE, warning=FALSE}
fit2d=lm(Y~ns(X,df=20),data=mcycle) 

plot(X ,Y, main= "Natural Cubic Spline with df=20", xlab="Times", ylab="Acceleration") 
lines(X, fit2d$fitted.values)

conf_interval <- predict(fit2d, interval="confidence",
                         level = 0.95)
lines(X, conf_interval[,2], col="red", lty=2)
lines(X, conf_interval[,3], col="red", lty=2)
```


## 2.3 Smoothing/Penalized Spline

We now use penalized splines where a penalty/smoothing parameter can help control the smoothness while many knots can be used and knot location does not need to be carefully selected. The s() function is part of the gam function from the mgcv package.

[go to top](#header)

# Generalized Additive Model
## 3.1 GAM using the "Motorcycle" dataset

```{r echo=TRUE, message = FALSE, warning= FALSE}
library(mgcv)
s_gam <- gam(Y ~ s(X),data=mcycle)
summary(s_gam)

#plot the model
plot(s_gam, residuals = TRUE, pch = 1)
```

## 3.2 GAM on "Wage" dataset

This dataset contains Wage and other data for a group of 3,000 male workers in the Mid-Atlantic region.
First we conduct EDA and fit a multiple linear regression of wage on age, year, and education.

```{r echo=TRUE, message = FALSE}
require(ISLR)
attach(Wage)
# ?Wage
dim(Wage)

s_Wage <- Wage[c("age","year","education","wage")]
str(s_Wage)
summary(s_Wage)
```


```{r echo=TRUE, message = FALSE}
library(ggplot2)
ggplot(data = Wage, mapping = aes(x = age, y = wage)) + 
    geom_point()

ggplot(data = Wage, mapping = aes(x = year, y = wage)) + 
    geom_point()

#categorical variable
ggplot(Wage, aes(x = education, y = wage)) +
    geom_boxplot()
```

```{r echo=TRUE, message = FALSE, results='hide'}
#multiple linear regression model
mul_model <- lm(wage~age+year+education, data=Wage)
summary(mul_model)
```


We then fit a GAM where 'age' and 'year' enter the model nonlinearly, whereas variable 'education' enters linearly.

```{r echo=TRUE, message = FALSE,warning=FALSE}
gam_mod <- gam(wage~s(age, k=6)+s(year, k=6)+education ,data = Wage)
summary(gam_mod)

plot(gam_mod, se=TRUE ,col="blue")
```


**Basis dimension choice for smooths**

*gam.check* runs a simple simulation based check on the basis dimensions, which can help to flag up terms for which k is too low. Grossly too small k will also be visible from partial residuals available with plot.gam.

```{r echo=TRUE, message = FALSE,warning=FALSE}
gam.check(gam_mod)
```

## 3.3 GAM on Boston Housing dataset

```{r echo=TRUE, message = FALSE,warning=FALSE, results='hide'}
library(MASS)
set.seed(1234)
sample_index <- sample(nrow(Boston),nrow(Boston)*0.70)
Boston_train <- Boston[sample_index,]
Boston_test <- Boston[-sample_index,]
str(Boston_train)
```


**Model and plots**


```{r echo=TRUE, message = FALSE,warning=FALSE}
#create gam model

Boston.gam <- gam(medv ~ s(crim)+s(zn)+s(indus)+chas+s(nox)
                 +s(rm)+s(age)+s(dis)+rad+s(tax)+s(ptratio)
                 +s(black)+s(lstat),data=Boston_train)

summary(Boston.gam)

plot(Boston.gam, pages=1)
```


**Model AIC/BIC and mean residual deviance**

```{r echo=TRUE, message = FALSE,warning=FALSE, results="hide"}
AIC(Boston.gam)
BIC(Boston.gam)
Boston.gam$deviance
```


**In-sample fit performance**

```{r echo=TRUE, message = FALSE,warning=FALSE}
#in-sample mse using df 
Boston.gam.mse.train <- Boston.gam$dev/Boston.gam$df.residual 
#Average Sum of Squared Error
Boston.gam.mse.train <- Boston.gam$dev/nrow(Boston_train) 

#using the predict() function
pi <- predict(Boston.gam,Boston_train)
mean((pi - Boston_train$medv)^2)
```

**out of sample performance**
```{r echo=TRUE, message = FALSE,warning=FALSE}
pi.out <- predict(Boston.gam,Boston_test)
mean((pi.out - Boston_test$medv)^2)
```

[go to top](#header)