---
title: "Generalized Additive Model"
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
    code_folding: show
    df_print: paged
editor_options: 
  chunk_output_type: console
---


# Generalized Additive Model

There are two common implementations of GAMs in R.  The older version (originally made for S-PLUS) is available as the 'gam' package by Hastie and Tibshirani.  The newer version that we will use below is the 'mgcv' package from Simon Wood. The basic modeling procedure for both packages is similar (the function is gam for both; be wary of having both libraries loaded at the same time), but the behind-the-scenes computational approaches differ, as do the arguments for optimization and the model output. Expect the results to be slightly different when used with the same model structure on the same dataset.

## GAM on Boston Housing dataset

```{r echo=TRUE, message = FALSE,warning=FALSE, results='hide'}
library(MASS)
set.seed(1234)
sample_index <- sample(nrow(Boston),nrow(Boston)*0.70)
Boston_train <- Boston[sample_index,]
Boston_test <- Boston[-sample_index,]
str(Boston_train)
```


**Model and plots**


```{r echo=TRUE, message = FALSE, warning=FALSE}
library(mgcv)

#create gam model
Boston.gam <- gam(medv ~ s(crim)+s(zn)+s(indus)+chas+s(nox)
                 +s(rm)+s(age)+s(dis)+rad+s(tax)+s(ptratio)
                 +s(black)+s(lstat),data=Boston_train)

summary(Boston.gam)

plot(Boston.gam, pages=1)
```


**Model AIC/BIC and mean residual deviance**

```{r echo=TRUE, message = FALSE,warning=FALSE, results="hide"}
AIC(Boston.gam)
BIC(Boston.gam)
Boston.gam$deviance
```


**In-sample fit performance**

```{r echo=TRUE, message = FALSE,warning=FALSE}
#in-sample mse using df 
Boston.gam.mse.train <- Boston.gam$dev/Boston.gam$df.residual 
#Average Sum of Squared Error
Boston.gam.mse.train <- Boston.gam$dev/nrow(Boston_train) 

#using the predict() function
pi <- predict(Boston.gam,Boston_train)
mean((pi - Boston_train$medv)^2)
```

**out of sample performance**
```{r echo=TRUE, message = FALSE,warning=FALSE}
pi.out <- predict(Boston.gam,Boston_test)
mean((pi.out - Boston_test$medv)^2)
```

[go to top](#header)


## GAM on Credit Scoring Data

```{r , message=FALSE}
credit_data <- read.csv(file = "https://xiaoruizhu.github.io/Data-Mining-R/lecture/data/credit_default.csv", header=T)

# rename
library(dplyr)
credit_data<- rename(credit_data, default=default.payment.next.month)
# convert categorical data to factor
credit_data$SEX<- as.factor(credit_data$SEX)
credit_data$EDUCATION<- as.factor(credit_data$EDUCATION)
credit_data$MARRIAGE<- as.factor(credit_data$MARRIAGE)

```

Now split the data 90/10 as training/testing datasets:

```{r}
index <- sample(nrow(credit_data),nrow(credit_data)*0.90)
credit_train = credit_data[index,]
credit_test = credit_data[-index,]
```

The training dataset has 24 variables, 10800 obs. 

You are already familiar with the credit scoring set. Let's define a cost function for benchmarking testing set performance. Note this is slightly different from the one we used for searching for optimal cut-off probability in logistic regression. Here the 2nd argument is the predict class instead of the predict probability (since many methods are not based on predict probability).

```{r}
creditcost <- function(observed, predicted){
  weight1 = 5
  weight0 = 1
  c1 = (observed==1)&(predicted == 0) #logical vector - true if actual 1 but predict 0
  c0 = (observed==0)&(predicted == 1) #logical vector - true if actual 0 but predict 1
  return(mean(weight1*c1+weight0*c0))
}
```

```{r, message=FALSE}
## Create a formula for a model with a large number of variables:
gam_formula <- as.formula(paste("default~s(LIMIT_BAL)+s(AGE)+s(PAY_0)+s(BILL_AMT1)+s(PAY_AMT1)+", 
                                paste(colnames(credit_train)[c(2:4)], collapse= "+")))

credit_gam <- gam(formula = gam_formula, family=binomial,data=credit_train);
summary(credit_gam)

plot(credit_gam, shade=TRUE, seWithMean=TRUE, scale=0, pages = 1)
# vis.gam(credit_gam)
vis.gam(credit_gam, view=c("LIMIT_BAL","AGE"), theta= 140) # different view 
```

### In-sample fit performance
In order to see the in-sample fit performance, you may look into the confusion matrix by using commands as following. 

```{r}
pcut_gam <- .15
prob_gam_in <-predict(credit_gam,credit_train,type="response")
pred_gam_in <- (prob_gam_in>=pcut_gam)*1
table(credit_train$default, pred_gam_in,dnn=c("Observed","Predicted"))
```

Likewise, misclassification rate is another thing you can check:

```{r}
mean(ifelse(credit_train$default != pred_gam_in, 1, 0))
```

Training model AIC, BIC, and mean residual deviance:
```{r}
AIC(credit_gam)
BIC(credit_gam)
credit_gam$deviance
```

### Search for optimal cut-off probability

The following code does a grid search from pcut = 0.01 to pcut = 0.99 with the objective of minimizing overall cost in the training set. I am using an asymmetric cost function by assuming that giving out a bad loan cost 10 time as much as rejecting application from someone who can pay.

```{r, fig.width=7}
#define the searc grid from 0.01 to 0.20
searchgrid = seq(0.01, 0.20, 0.01)
#result_gam is a 99x2 matrix, the 1st col stores the cut-off p, the 2nd column stores the cost
result_gam = cbind(searchgrid, NA)
#in the cost function, both r and pi are vectors, r=Observed, pi=predicted probability
cost1 <- function(r, pi){
  weight1 = 5
  weight0 = 1
  c1 = (r==1)&(pi<pcut) #logical vector - true if actual 1 but predict 0
  c0 = (r==0)&(pi>pcut) #logical vector - true if actual 0 but predict 1
  return(mean(weight1*c1+weight0*c0))
}

for(i in 1:length(searchgrid))
{
  pcut <- result_gam[i,1]
  #assign the cost to the 2nd col
  result_gam[i,2] <- cost1(credit_train$default, predict(credit_gam,type="response"))
}
plot(result_gam, ylab="Cost in Training Set")
index_min <- which.min(result_gam[,2])#find the index of minimum value
result_gam[index_min,2] #min cost
result_gam[index_min,1] #optimal cutoff probability
```

### Out-of-sample fit performance
```{r}
pcut <-  result_gam[index_min,1] 
prob_gam_out <- predict(credit_gam, credit_test,type="response")
pred_gam_out <- (prob_gam_out>=pcut)*1
table(credit_test$default, pred_gam_out,dnn=c("Observed","Predicted"))
```

The mis-classifciation rate is
```{r}
mean(ifelse(credit_test$default != pred_gam_out, 1, 0))
```

The cost associated with misclassification is
```{r}
creditcost(credit_test$default, pred_gam_out)
```

[go to top](#header)


## GAM using the "Motorcycle" dataset

```{r echo=TRUE, message = FALSE, results='hide'}
library(MASS)
data('mcycle')
str(mcycle)
summary(mcycle)
```

```{r echo=TRUE, message = FALSE}
# Rename the variables for ease of usage
Y <- mcycle$accel
X <- mcycle$times

#Scatterplot
plot(Y~X, xlab="time",ylab="Acceleration", main="Scatterplot of Acceleration against Time")
```

```{r echo=TRUE, message = FALSE, warning= FALSE}
library(mgcv)
s_gam <- gam(Y ~ s(X),data=mcycle)
summary(s_gam)

#plot the model
plot(s_gam, residuals = TRUE, pch = 1)
```

[go to top](#header)

## GAM on "Wage" dataset

This dataset contains Wage and other data for a group of 3,000 male workers in the Mid-Atlantic region.
First we conduct EDA and fit a multiple linear regression of wage on age, year, and education.

```{r echo=TRUE, message = FALSE}
require(ISLR)
attach(Wage)
# ?Wage
dim(Wage)

s_Wage <- Wage[c("age","year","education","wage")]
str(s_Wage)
summary(s_Wage)
```

```{r echo=TRUE, message = FALSE}
library(ggplot2)
ggplot(data = Wage, mapping = aes(x = age, y = wage)) + 
    geom_point()

ggplot(data = Wage, mapping = aes(x = year, y = wage)) + 
    geom_point()

#categorical variable
ggplot(Wage, aes(x = education, y = wage)) +
    geom_boxplot()
```

```{r echo=TRUE, message = FALSE, results='hide'}
#multiple linear regression model
mul_model <- lm(wage~age+year+education, data=Wage)
summary(mul_model)
```


We then fit a GAM where 'age' and 'year' enter the model nonlinearly, whereas variable 'education' enters linearly.

```{r echo=TRUE, message = FALSE,warning=FALSE}
gam_mod <- gam(wage~s(age, k=6)+s(year, k=6)+education ,data = Wage)
summary(gam_mod)

plot(gam_mod, se=TRUE ,col="blue")
```


**Basis dimension choice for smooths**

*gam.check* runs a simple simulation based check on the basis dimensions, which can help to flag up terms for which k is too low. Grossly too small k will also be visible from partial residuals available with plot.gam.

```{r echo=TRUE, message = FALSE,warning=FALSE}
gam.check(gam_mod)
```

[go to top](#header)

