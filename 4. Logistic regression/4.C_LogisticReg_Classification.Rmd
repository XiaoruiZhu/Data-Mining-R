---
title: "Logistic Regression for Binary Classification"
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
  html_notebook: 
    fig_caption: yes
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Objective

The objective of this case is to get you understand logistic regression (binary classification) and some important ideas such as cross validation, ROC curve, cut-off probability. 

# Credit Card Default Data

We will use a Credit Card Default Data for this lab and illustration. The details of the data can be found at http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients. 
Think about what kind of factors could affect people to fail to pay their credit balance.

We first load the credit scoring data. It is easy to load comma-separated values (CSV). 
```{r}
credit_data <- read.csv(file = "https://xiaoruizhu.github.io/Data-Mining-R/lecture/data/credit_default.csv", header=T)
```

Look at what information do we have.
```{r}
colnames(credit_data)
```

Let's look at how many people were actually default in this sample.
```{r}
mean(credit_data$default.payment.next.month)
```

The name of response variable is too long! I want to make it shorter by renaming. Recall the `rename()` function.
```{r message=FALSE}
library(dplyr)
credit_data<- rename(credit_data, default=default.payment.next.month)
```

How about the variable type and summary statistics?
```{r eval=FALSE}
str(credit_data)    # structure - see variable type
summary(credit_data) # summary statistics
```

We see all variables are **int**, but we know that *SEX, EDUCATION, MARRIAGE* are categorical, we convert them to **factor**.
```{r}
credit_data$SEX<- as.factor(credit_data$SEX)
credit_data$EDUCATION<- as.factor(credit_data$EDUCATION)
credit_data$MARRIAGE<- as.factor(credit_data$MARRIAGE)
```

*We omit other EDA, but you shouldn't whenever you are doing data analysis.*

[go to top](#header)


# Logistic Regression

Randomly split the data to training (80%) and testing (20%) datasets:
```{r}
index <- sample(nrow(credit_data),nrow(credit_data)*0.80)
credit_train = credit_data[index,]
credit_test = credit_data[-index,]
```

## Train a logistic regression model with all variables

```{r, warning=FALSE}
credit_glm0<- glm(default~., family=binomial, data=credit_train)
summary(credit_glm0)
```

You have seen `glm()` before. In this lab, this is the main function used to build logistic regression model because it is a member of generalized linear model. In `glm()`, the only thing new is `family`. It specifies the distribution of your response variable. You may also specify the link function after the name of distribution, for example, `family=binomial(logit)` (default link is logit). You can also specify `family=binomial(link = "probit")` to run probit regression. You may also use `glm()` to build many other generalized linear models.

## Binary Classification

As we talked in the lecture, people may be more interested in the classification results. But we have to define a cut-off probability first.

These tables illustrate the impact of choosing different cut-off probability. Choosing a large cut-off probability will result in few cases being predicted as 1, and chossing a small cut-off probability will result in many cases being predicted as 1.
```{r}

pred_glm0_train <- predict(credit_glm0, type="response")

table((pred_glm0_train > 0.9)*1)
table((pred_glm0_train > 0.5)*1)
table((pred_glm0_train > 0.2)*1)
table((pred_glm0_train > 0.0001)*1)
```
Therefore, determine the optimal cut-off probability is crucial. The simplest way to determine the cut-off is to use the proportion of "1" in the original data. We will intriduce a more appropriate way to determine the optimal p-cut.

#### **Exercise 1:** 

> 1. Change the weights to different values, and see how your optimal cut-off changes.
> 2. obtain confusion matrix and calculate the (asymmetric) cost based on the optimal cut-off. 
> 3. Find optimal cut-off probability using symmetric cost. 
> 4. Calculate MR and cost, what do you find?
> 5. Further, rewrite the cost function to make the weights (or the ratio of two weights) as input parameter.
> 6. Use F-score to determine the optimal cut-off.

### Out-of-sample Classification

Everything you have done about classification so far is for training sample. Now let's get to testing sample. Keep in mind the principle, **testing sample is only used for evaluating your model's prediction accuracy**! NO NEED TO CHOOSE CUT-OFF PROBABILITY in this stage.

#### **Exercise 2:** 

> 1. Calculate MR, FPR, FNR based on the optimal cut-off you get from training sample with weights (5:1)
> 2. Calculate asymetric cost based on the optimal cut-off you get from training sample with weights (5:1)
> 3. Calculate above statistics based on the cut-off you get from training sample with symmetric weights (1:1) 

**************************


# Summary

## Things to remember

* Know how to use glm() to build logistic regression;

* Know how to get ROC and AUC based on predicted probability;

* Know how to get PR curve and AUC based on predicted probability;

* Know how to find optimal cut-off;

* Know how to do binary classification, and calculation of MR, FPR, FNR, and cost;

* Know how to use LASSO for logistic regression

## Guide for Assignment

* EDA

* Train logistic model

* Prediction (ROC, AUC; PR, AUC)

* Model comparison using AUC

* Find optimal cut-off based on training sample

* Classification -- Obtain the confusion matrix and calculate the MR, asymmetric cost, FPR, Precision, Recall, and F-score for both training and testing sample based on (1) naive cut-off determined by sample proportion; (2) optimal cut-off determined by asymmetric cost; (3) optimal cut-off determined by F-score.

* Build new models by Variable Selection 

* Calculate all criteria

* Comprehensive comparison

[go to top](#header)

