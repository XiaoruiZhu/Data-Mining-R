---
title: "Logistic Regression, Prediction and ROC"
output: 
  html_document: 
    theme: readable
    fig_caption: yes
    number_sections: yes
    toc: yes
  html_notebook: 
    fig_caption: yes
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: console
---

# Objective

The objective of this case is to get you understand logistic regression (binary classification) and some important ideas such as cross validation, ROC curve, cut-off probability. Code in this case is built upon lecture slides and sample code of Shaonan Tian and Shaobo Li.

# Input and sample data

We will use a Credit Card Default Data for this lab and illustration. The details of the data can be found at http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients. 
Think about what kind of factors could affect people to fail to pay their credit balance.

First load the [credit scoring data](data/credit_default.csv). It is easy to load comma-separated values (CSV). 
```{r}
credit.data <- read.csv(file = "data/credit_default.csv", header=T)
```

Look at what information do we have.
```{r}
colnames(credit.data)
```

Let's look at how many people were actually default in this sample.
```{r}
mean(credit.data$default.payment.next.month)
```

The name of response variable is too long! I want to make it shorter by renaming. Recall the `rename()` function.
```{r message=FALSE}
library(dplyr)
credit.data<- rename(credit.data, default=default.payment.next.month)
```

How about the variable type and summary statistics?
```{r eval=FALSE}
str(credit.data)    # structure - see variable type
summary(credit.data) # summary statistics
```

We see all variables are **int**, but we know that *SEX, EDUCATION, MARRIAGE* are categorical, we convert them to **factor**.
```{r}
credit.data$SEX<- as.factor(credit.data$SEX)
credit.data$EDUCATION<- as.factor(credit.data$EDUCATION)
credit.data$MARRIAGE<- as.factor(credit.data$MARRIAGE)
```

[go to top](#header)

# Two-way contingency table and Chi-square test

Two-way contingency table is a very useful tool for exploring the relationship between categorical variables. It is essentially the simplest pivot-table (see example below). Often time, after you create a two-way contingency table, Chi-square test is used to test if X affect Y. The null hypothesis is: X and Y are independent (e.g., MARRIAGE has nothing to do with likelihood of default).

The $\chi^2$ test statistic is defined as
$$\chi^2=\sum \frac{(observed-expected)^2}{expected}$$,
where the expected count is calculated by assuming row variable has nothing to do with column variable.

Here is a very good tutorial for Chi-square test https://www.youtube.com/watch?v=WXPBoFDqNVk. 

```{r}
table.edu<- table(credit.data$EDUCATION, credit.data$default)
table.edu
chisq.test(table.edu)
```

What we saw from above test result is that p-value < 0.05. What is your conclusion?


*We omit other EDA, but you shouldn't whenever you are doing data analysis.*

# Logistic Regression
Randomly split the data to training (80%) and testing (20%) datasets:
```{r}
index <- sample(nrow(credit.data),nrow(credit.data)*0.80)
credit.train = credit.data[index,]
credit.test = credit.data[-index,]
```

## Train a logistic regression model with all X variables

```{r, warning=FALSE}
credit.glm0<- glm(default~., family=binomial, data=credit.train)
summary(credit.glm0)
```

You have seen `glm()` before. In this lab, this is the main function used to build logistic regression model because it is a member of generalized linear model. In `glm()`, the only thing new is `family`. It specifies the distribution of your response variable. You may also specify the link function after the name of distribution, for example, `family=binomial(logit)` (default link is logit). You can also specify `family=binomial(link = "probit")` to run probit regression. You may also use `glm()` to build many other generalized linear models.

## Get some criteria of model fitting

You can simply extract some criteria of the model fitting, for example, Residual deviance (equivalent to SSE in linear regression model), AIC and BIC. Unlike linear regression models, there is no $R^2$ in logistic regression.
```{r}
credit.glm0$deviance
AIC(credit.glm0)
BIC(credit.glm0)
```

## Prediction

Similar to linear regression, we use `predict()` function for prediction. 

To get prediction from a logistic regression model, there are several steps you need to understand. Refer to textbook/slides for detailed math.

1.The fitted model $\hat{\eta} = b_0 +b_1 x_1 + b_2 x_2 + ...$ gives you the estimated value before the inverse of link (logit in case of logistic regression). In logistic regression the $\hat{\eta}$ are called **log odds ratio**, which is $\log(P(y=1)/(1-P(y=1)))$. In R you use the *predict()* function to get a vector of all in-sample $\hat{\eta}$ (for each training obs).

```{r, fig.width=4, fig.height=4}
hist(predict(credit.glm0))
```

2.For each $\hat{\eta}$, in order to get the P(y=1), we can apply the inverse of the link function (logit here) to $\hat{\eta}$. The equation is $P(y=1) =  1/ (1+exp(-\hat{\eta}))$. In R you use the *fitted()* function or *predict(,type="response") to get the **predicted probability** for each training ob.
```{r, fig.width=4, fig.height=4}
hist(predict(credit.glm0,type="response"))
```

3.Last but not least, you want a binary classification decision rule. The default rule is if the fitted $P(y=1) > 0.5$ then $y = 1$. The value 0.5 is called **cut-off probability**. You can choose the cut-off probability based on mis-classification rate, cost function, etc. In this case, the cost function can indicate the trade off between the risk of giving loan to someone who cannot pay (predict 0, truth 1), and risk of rejecting someone who qualifys (predict 1, truth 0).

These tables illustrate the impact of choosing different cut-off probability. Choosing a large cut-off probability will result in few cases being predicted as 1, and chossing a small cut-off probability will result in many cases being predicted as 1.

```{r}
table(predict(credit.glm0,type="response") > 0.5)
table(predict(credit.glm0,type="response") > 0.2)
table(predict(credit.glm0,type="response") > 0.0001)
```

### In-sample prediction (less important)
```{r}
pred.glm0.train<- predict(credit.glm0, type="response")
```

(**IMPORTANT!!!**) You have to specify `type="response"` in order to get probability outcome, which is what we want. Otherwise, what it produces is the linear predictor term $\beta_0+\beta_1X_1+\beta_2X_2+\dotso$. Recall the lecture, how is this linear predictor related to probability?

#### ROC Curve

In order to show give an overall measure of goodness of classification, using the Receiver Operating Characteristic (ROC) curve is one way. Rather than use an overall misclassification rate, it employs
two measures â€“ true positive fraction (TPF) and false positive fraction (FPF). 

True positive fraction, $\text{TPF}=\frac{\text{TP}}{\text{TP+FN}}$: is the proportion of true positives correctly predicted as positive.

False positive fraction, $\text{FPF}=\frac{\text{FP}}{\text{FP+TN}}=1-\frac{\text{TN}}{\text{FP+TN}}$: is the proportion of true negatives incorrectly predicted as positive.

![](pic/ClassTable.PNG)

![](pic/ROC_curve.PNG)

```{r, eval=FALSE}
install.packages('ROCR')
```
```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
library(ROCR)
pred <- prediction(pred.glm0.train, credit.train$default)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
```

Be careful that the function `prediction()` is different from `predict()`. It is in Package `ROCR`, and is particularly used for preparing for ROC curve. Recall out lecture, this function basically calculates many confusion matrices with different cut-off probability. Therefore, it requires two vectors as inputs -- **predicted probability** and **observed response (0/1)**. The next line, `performance()` calculates TPR and FPR based all confusion matrices you get from previous step. Then you can simply draw the ROC curve, which is a curve of FPR vs. TPR. The last line is to get AUC (area under the curve). I would recommend you to stick these four lines of code together, and use it to get ROC curve and AUC. If you don't want to draw the ROC curve (because it takes time), just comment out plot line.

#### Precision-Recall Curve

Precision and recall curve and its AUC is more appropriate for imbalanced data. We use package `PRROC` to draw the PR curve. It can also draw the ROC curve. More details of the package can be found [here](https://cran.r-project.org/web/packages/PRROC/vignettes/PRROC.pdf).

```{r eval=FALSE}
install.packages("PRROC")
```
```{r message=FALSE, warning=FALSE}
library(PRROC)
score1= pred.glm0.train[credit.train$default==1]
score0= pred.glm0.train[credit.train$default==0]
roc= roc.curve(score1, score0, curve = T)
roc$auc
pr= pr.curve(score1, score0, curve = T)
pr
plot(pr)
```


### Out-of-sample prediction (more important)
```{r}
pred.glm0.test<- predict(credit.glm0, newdata = credit.test, type="response")
```

For out-of-sample prediction, you have to specify `newdata="testing sample name"`.

#### ROC Curve
```{r, message=FALSE, warning=FALSE, fig.width=6, fig.height=5}
pred <- prediction(pred.glm0.test, credit.test$default)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
#Get the AUC
unlist(slot(performance(pred, "auc"), "y.values"))
```

#### Precision-Recall Curve
```{r}
score1.test= pred.glm0.test[credit.test$default==1]
score0.test= pred.glm0.test[credit.test$default==0]
roc.test= roc.curve(score1.test, score0.test, curve = T)
roc.test$auc
pr.test= pr.curve(score1.test, score0.test, curve = T)
pr.test
plot(pr.test)
```

## Binary Classification

As we talked in the lecture, people may be more interested in the classification results. But we have to define a cut-off probability first.

These tables illustrate the impact of choosing different cut-off probability. Choosing a large cut-off probability will result in few cases being predicted as 1, and chossing a small cut-off probability will result in many cases being predicted as 1.
```{r}
table((pred.glm0.train > 0.9)*1)
table((pred.glm0.train > 0.5)*1)
table((pred.glm0.train > 0.2)*1)
table((pred.glm0.train > 0.0001)*1)
```
Therefore, determine the optimal cut-off probability is crucial. The simplest way to determine the cut-off is to use the proportion of "1" in the original data. We will intriduce a more appropriate way to determine the optimal p-cut.

### Naive Choice of Cut-off probability

The simplest way is to choose the event proportion in training sample. This is roughly reasonable because the sample proportion is an estimate of mean probability of $Y=1$. 
```{r}
pcut1<- mean(credit.train$default)
```

Based on this cut-off probability, we can obtain the binary prediction (predicted classification) and the confusion matrix
```{r}
# get binary prediction
class.glm0.train<- (pred.glm0.train>pcut1)*1
# get confusion matrix
table(credit.train$default, class.glm0.train, dnn = c("True", "Predicted"))
```
In `table()` function, two vectors must be both binary in order to get confusion matrix (it is essentially a pivot table or contingency table), `dnn` is to specify the row and column name of this 2*2 table. The first input vector is TRUE, so the first name should be TRUE accordingly.

Then it is easy to get different types of classification error rate, i.e., false positive rate (FPR), false negative rate (FNR), and overall misclassification rate (MR). **Commonly, you can use overall MR as the cost (a criterion) to evaluate the model prediction.**
```{r, eval=FALSE}
# (equal-weighted) misclassification rate
MR<- mean(credit.train$default!=class.glm0.train)
# False positive rate
FPR<- sum(credit.train$default==0 & class.glm0.train==1)/sum(credit.train$default==0)
# False negative rate (exercise)
FNR<- 
```

### Determine Optimal cut-off Probability using Grid Search Method
Recall the lecture, different p-cut results in different confusion matrix, hence different MR (or cost). You need to search all possible p-cut to find the one that provides minimum cost. The first step is to define a symmetric/asymmetric cost function (misclassification rate), as a function of cut-off. Think about what else is needed to calculate MR? The answer is observed $Y$ and predicted probability.
```{r}
# define a cost function with input "obs" being observed response 
# and "pi" being predicted probability, and "pcut" being the threshold.
costfunc = function(obs, pred.p, pcut){
	weight1 = 5   # define the weight for "true=1 but pred=0" (FN)
	weight0 = 1    # define the weight for "true=0 but pred=1" (FP)
	c1 = (obs==1)&(pred.p<pcut)    # count for "true=1 but pred=0"   (FN)
	c0 = (obs==0)&(pred.p>=pcut)   # count for "true=0 but pred=1"   (FP)
	cost = mean(weight1*c1 + weight0*c0)  # misclassification with weight
	return(cost) # you have to return to a value when you write R functions
} # end of the function
```
Next, define a sequence of probability (you need to search the optimal p-cut from this sequence)
```{r}
# define a sequence from 0.01 to 1 by 0.01
p.seq = seq(0.01, 1, 0.01) 
```
Then, you need to calculate the cost (as you defined before) for each probability in the sequence p.seq.
```{r}
# write a loop for all p-cut to see which one provides the smallest cost
# first, need to define a 0 vector in order to save the value of cost from all pcut
cost = rep(0, length(p.seq))  
for(i in 1:length(p.seq)){ 
	cost[i] = costfunc(obs = credit.train$default, pred.p = pred.glm0.train, pcut = p.seq[i])  
} # end of the loop
```
Last, draw a plot with cost against p.seq, and find the p-cut that gives you the minimum cost.
```{r}
# draw a plot with X axis being all pcut and Y axis being associated cost
plot(p.seq, cost)
# find the optimal pcut
optimal.pcut.glm0 = p.seq[which(cost==min(cost))]
```

#### Use the optimal cut-off probability

Now let calculate MR, FPR, FNR, and cost based on the optimal cut-off.

```{r}
# step 1. get binary classification
class.glm0.train.opt<- (pred.glm0.train>optimal.pcut.glm0)*1
# step 2. get confusion matrix, MR, FPR, FNR
table(credit.train$default, class.glm0.train.opt, dnn = c("True", "Predicted"))
MR<- mean(credit.train$default!= class.glm0.train.opt)
FPR<- sum(credit.train$default==0 & class.glm0.train.opt==1)/sum(credit.train$default==0)
FNR<- sum(credit.train$default==1 & class.glm0.train.opt==0)/sum(credit.train$default==1)
cost<- costfunc(obs = credit.train$default, pred.p = pred.glm0.train, pcut = optimal.pcut.glm0)  
```


#### Exercise: 

* Change the weights to different values, and see how your optimal cut-off changes.

* obtain confusion matrix and calculate the (asymmetric) cost based on the optimal cut-off. 

* Find optimal cut-off probability using symmetric cost. 

* Calculate MR and cost, what do you find?

* Further, rewrite the cost function to make the weights (or the ratio of two weights) as input parameter.

* Use F-score to determine the optimal cut-off.

### Out-of-sample Classification

Everything you have done about classification so far is for training sample. Now let's get to testing sample. Keep in mind the principle, **testing sample is only used for evaluating your model's prediction accuracy**! NO NEED TO CHOOSE CUT-OFF PROBABILITY in this stage.

#### Exercise:

* Calculate MR, FPR, FNR based on the optimal cut-off you get from training sample with weights (5:1)

* Calculate asymetric cost based on the optimal cut-off you get from training sample with weights (5:1)

* Calculate above statistics based on the cut-off you get from training sample with symmetric weights (1:1) 


**************************

## Variable Selection 

### Variable Selection with Stepwise Approach
We can use the same procedures of variable selection, i.e. forward, backward, and stepwise, for linear regression models. **caution: this will take a long time since the sample size is not small**.
```{r, eval=FALSE}
credit.glm.back <- step(credit.glm0) # backward selection (if you don't specify anything)
summary(credit.glm.back)
credit.glm.back$deviance
AIC(credit.glm.back)
BIC(credit.glm.back)
```

You can try model selection with BIC (usually results in a simpler model than AIC criterion)
```{r, eval= FALSE}
credit.glm.back.BIC <- step(credit.glm0, k=log(nrow(credit.train))) 
summary(credit.glm.back.BIC)
credit.glm.back.BIC$deviance
AIC(credit.glm.back.BIC)
BIC(credit.glm.back.BIC)
```
**Exercise:** Try forward and stepwise selection procedures to see if they deliver the same best model.

#### Exercise:

* Get ROC curve and AUC for both training and testing sample

* Using training sample, find the optimal cut-off by grid search method with asymmetric cost (weights ratio = 5:1)

* Calculate MR, FPR, FNR, the asymmetric cost for both taining and testing sample.


**************************


### Variable selection with LASSO

Be careful that LASSO does require x to be **numeric** matrix. Therefore, we need to manually convert categorical variable ("SEX", "EDUCATION" and "MARRIAGE") to dummy variable. For simplicity, only if you have evidence that the categorical variable has monotonic relationship to response can you directly convert it to numeric by using `as.numeric()`. For example, the probability of default increases/decreases as EDUCATION level goes from 1 to 4. This can be seen from the two-way contingency table by calculating the default proportion at each education level. 

Here I will show how to convert categorical variable to dummy variables.

```{r}
dummy<- model.matrix(~ ., data = credit.data)
```
```{r eval=FALSE}
# look at first few rows of data
head(dummy)
```

The function `model.matrix()` can automatically convert categorical variable to dummy. It also creates a column of 1, which we don't need at this time. That column of 1 is used for estimating intercept if you write algorithm by yourself, but most available functions automatically creates that column during estimation. 

```{r}
credit.data.lasso<- data.frame(dummy[,-1])
```

Now let's get data prepared for LASSO.
```{r}
#index <- sample(nrow(credit.data),nrow(credit.data)*0.80)
credit.train.X = as.matrix(select(credit.data.lasso, -default)[index,])
credit.test.X = as.matrix(select(credit.data.lasso, -default)[-index,])
credit.train.Y = credit.data.lasso[index, "default"]
credit.test.Y = credit.data.lasso[-index, "default"]
```


```{r}
library(glmnet)
credit.lasso<- glmnet(x=credit.train.X, y=credit.train.Y, family = "binomial")
```

Perform cross-validation to determine the shrinkage parameter.
```{r}
credit.lasso.cv<- cv.glmnet(x=credit.train.X, y=credit.train.Y, family = "binomial", type.measure = "class")
plot(credit.lasso.cv)
```

For logistc regression, we can specify `type.measure="class"` so that the CV error will be misclassification error.

Get the coefficient with optimal $\lambda$
```{r}
coef(credit.lasso, s=credit.lasso.cv$lambda.min)
coef(credit.lasso, s=credit.lasso.cv$lambda.1se)
```

#### Prediction
```{r}
# in-sample prediction
pred.lasso.train<- predict(credit.lasso, newx=credit.train.X, s=credit.lasso.cv$lambda.1se, type = "response")
# out-of-sample prediction
pred.lasso.test<- predict(credit.lasso, newx=credit.test.X, s=credit.lasso.cv$lambda.1se, type = "response")
```

#### Exercise:

* Get ROC curve and AUC for both training and testing sample

* Using training sample, find the optimal cut-off by grid search method with asymmetric cost (weights ratio = 5:1)

* Calculate MR, FPR, FNR, the asymmetric cost for both taining and testing sample.


## Cross validation
Refer to lecture slides and Elements of Statistical Learning book (section 7.10) for more advice on cross validation.
```{r}
pcut = 0.5
#Symmetric cost
cost1 <- function(r, pi, pcut){
  mean(((r==0)&(pi>pcut)) | ((r==1)&(pi<pcut)))
}

#Asymmetric cost
cost2 <- function(r, pi, pcut){
  weight1 = 2
  weight0 = 1
  c1 = (r==1)&(pi<pcut) #logical vector - true if actual 1 but predict 0
  c0 = (r==0)&(pi>pcut) #logical vector - true if actual 0 but predict 1
  return(mean(weight1*c1+weight0*c0))
}
```

You should read the cv.glm help to understand how it works.
We can use the same cost function as defined before, but you need to modify it such that there are only two input: observed $Y$ and predicted probability.
```{r}
costfunc = function(obs, pred.p){
	weight1 = 5   # define the weight for "true=1 but pred=0" (FN)
	weight0 = 1    # define the weight for "true=0 but pred=1" (FP)
	c1 = (obs==1)&(pred.p<pcut)    # count for "true=1 but pred=0"   (FN)
	c0 = (obs==0)&(pred.p>=pcut)   # count for "true=0 but pred=1"   (FP)
	cost = mean(weight1*c1 + weight0*c0)  # misclassification with weight
	return(cost) # you have to return to a value when you write R functions
} # end of the function
```
Then you need to assign a value to pcut.
```{r}
pcut = optimal.pcut.glm0  
```

10-fold cross validation, note you should use the **full data** for cross-validation. In `cv.glm`, default cost function is the average squared error function.

```{r, message=FALSE, warning=FALSE}
library(boot)
credit.glm1<- glm(default~. , family=binomial, data=credit.data);  
cv.result = cv.glm(data=credit.data, glmfit=credit.glm1, cost=costfunc, K=10) 
cv.result$delta[2]
```

The first component of `delta` is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.

Keep in mind that CV-score is averaged model error. Here, it is the cost you have defined before. You may also use F-score for cross-validation, but again, you need to define the function of F-score. (Exercise!)

[go to top](#header)


# Summary

## Things to remember

* Know how to use glm() to build logistic regression;

* Know how to get ROC and AUC based on predicted probability;

* Know how to get PR curve and AUC based on predicted probability;

* Know how to find optimal cut-off;

* Know how to do binary classification, and calculation of MR, FPR, FNR, and cost;

* Know how to use LASSO for logistic regression

## Guide for Assignment

* EDA

* Train logistic model

* Prediction (ROC, AUC; PR, AUC)

* Model comparison using AUC

* Find optimal cut-off based on training sample

* Classification -- Obtain the confusion matrix and calculate the MR, asymmetric cost, FPR, Precision, Recall, and F-score for both training and testing sample based on (1) naive cut-off determined by sample proportion; (2) optimal cut-off determined by asymmetric cost; (3) optimal cut-off determined by F-score.

* Build new models by Variable Selection 

* Calculate all criteria

* Comprehensive comparison

[go to top](#header)

# Starter code for German credit scoring

Refer to http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data) for variable description.
Notice that "It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1)." Define your cost function accordingly!

```{r,eval=FALSE}
german_credit = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data")

colnames(german_credit)=c("chk_acct","duration","credit_his","purpose","amount","saving_acct","present_emp","installment_rate","sex","other_debtor","present_resid","property","age","other_install","housing","n_credits","job","n_people","telephone","foreign","response")

#orginal response coding 1= good, 2 = bad
#we need 0 = good, 1 = bad
german_credit$response = german_credit$response - 1
```

[go to top](#header)

```{r , echo=FALSE, eval=FALSE}
m <- 1000 # simulation times
coef <- c(-1.1, 5, -0.4)
set.seed(43657)
n <- 500
x1 <- runif(n, 0, 1)
x2 <- (rep(c(1,0), n/2))
x3 <- rnorm(n)

linearPart <- coef[1] + coef[2] * x1 + coef[3] * x2

p <- 1/(1+exp(-(linearPart)))
y <- rbinom(n, 1, p)
# summary(as.factor(y))
# m <- sum(y==1)
# hist(p)
# hist(p[y==1])
# hist(p[y==0])

dat <- data.frame(y, x1, x2, x1.2=x1^2, x3)
mod1 <- glm(y~x1+x2 , family=binomial, data = dat)
# summary(mod1)
pred1 <- predict(mod1, newdata = dat, type = "response")
dec1 <- decileT(pred.prob = pred1, OrigDefault = dat$y)
# dec1
# hist(pred1)
test1 <- gof(mod1, g = 10, plotROC = FALSE)
# test1$gof; 
# test1$auc
```

